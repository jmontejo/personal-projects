\section{Statement of research interest}
\cvitem{}{
After the successful observation of the Higgs boson in Run 1, the increased energy of Run 2 has provided the LHC experiments with an unprecedented dataset to explore the energy frontier. No significant excess has been observed so far, and stringent limits have been set on simplified models. 
The motivation for some form of new physics is still strong but it is obvious that it is not manifested in the vanilla signatures that were the main focus of attention. 
At this stage two different approaches can and should be pursued. On one side, searches in final states with low cross section or challenging signatures will become more relevant with the growing dataset. On the other side, precision measurements are critically needed and are extremely valuable results on their own. 
Especially important are measurements in final states with non-zero electric charge, since they can not be measured at future electron-positron colliders. 
}

\cvitem{}{
The ATLAS measurement program has been extremely successful, and processes with cross-sections as low as $\mathcal{O}(1)$ fb have been measured. However it is important to keep in mind that so far less than 5\% of the target of 3000 $\text{fb}^{-1}$ has been recorded. The large increase in luminosity will benefit especially processes with low cross-sections, and final states with high purity but low branching ratios, such as multi-lepton final states. In addition, it will also transform the treatment of objects where the choice of working point means a trade-off between purity and statistics. Examples of such are identification and isolation of leptons, or tagging of jets originating from $b$-quarks ($b$-jets). 
%Another example based on an ATLAS analysis is the current \ttH\ multi-lepton analysis, which has a background from fake or non-prompt lepton that accounts for about 40\% of the observed events in many of its best signal regions. Handles to suppress these reducible backgrounds exist but the low number of signal events requires an analysis with high acceptance. There is also non-negligible diboson background, which contains prompt isolated leptons but enters the analysis region mostly via mis-tagged jet, with only a small contribution from diboson plus heavy flavour jets. Both backgrounds 
}
\cvitem{}{
For the aforementioned reasons, I consider that a program of measurements in final states with multiple leptons and $b$-jets will become extremely valuable during the next years, and up to the high-luminosity phase of LHC (HL-LHC). 
}

%multilepton multib
%ttW measurement
%W+bb
%soft muon top mass???
%need something also more longer term
% ttW
\cvitem{ttW}{
One of the processes that has been measured for the first time at the LHC is the \ttW\ process. Several ATLAS results at 8 and 13 TeV have measured \ttW\ cross-sections which deviate from the SM prediction, and a better understanding of this process is essential. 
In particular unexpected correlations of the charge asymmetry with the jet and $b$-jet multiplicity are observed consistently, which require further study. 
Unfolded measurements of kinematic variables will be provided in the short future, however there are several reasons why a measurement of this process will be dramatically improved with the increase in luminosity of upcoming years:
\begin{itemize}
\item The increase of statistics will allow for an analysis with a tighter lepton identification and isolation, which could reduce to a negligible level the background from fake and non-prompt leptons, almost doubling the signal purity in several regions. The remaining background originates mostly from \ttZ\ with a lost lepton, which can be measured very precisely.
\item Given the unexpected correlations seen with the charge asymmetry, 2-dimensional measurements of the charge asymmetry with the number or jets, or number of $b$-jets, will be required to study this effect. Such multi-dimensional measurements are not possible without higher luminosity.
\end{itemize}
}
\cvitem{}{
\begin{itemize}
\item A requirement of two $b$-tags in the event (currently restricted to one $b$-tag) will reduce the combinatorial problem when pairing leptons and jets to build observables closely related to top-quark kinematics. It will also allow a cleaner measurement of the production of additional jets, especially in the three-lepton category. Improving the resolution of the measurement, and reconstructing correctly observables that link directly to the top quark kinematics, are critical improvements in a final state where the presence of multiple neutrinos prevents the full reconstruction exploiting the known mass constraints.
\end{itemize}
As discussed, the larger dataset of the HL-LHC will certainly benefit most of the analyses, but it has the potential to really transform the \ttW\ measurement.
}

% V+bb
\cvitem{W+bb}{
%\cvitem{\textbf{}}{}{}{}{
The measurement of a $W$ boson in association with $b$-jets is an extremely challenging one due to the massive background from \ttbar\ production. The lepton charge asymmetry can be exploited to separate out this process, however it is then again drowned by single-top production in $t$-channel, which produces a similar final state with a $W$-boson, $b$-jets, and is also charge asymmetric. These challenges have led to only one ATLAS measurement of this process, at 7 TeV measuring the inclusive $W$+b cross-section, and a differential measurement of $W$+b together with single top.
This process represents also the main background to $WH(b\bar{b})$, and becomes increasingly relevant now that Higgs measurement move towards the measurement of simplified template cross-sections at high $p_T(H)$, leading to close-by $b$-jets. In this topology the contribution of $W$+bb, where the $b$-jet pair originates from gluon splitting, is strongly enhanced while the \ttbar\ background is suppressed given that the $b$-jets tend to be aligned back-to-back. This collinear regime carries an additional intrinsic interest since MC generators are usually not able to model it correctly.
}
\cvitem{}{
%Valuable by itself, probe of gluon fragmentation. Largely unconstrained at small opening angles
I am interested in a measurement of the $W$+bb process, focusing in the regime with two close-by $b$-jets. In order to overcome the intrinsic limitation of the radius of R=0.4 that is used in jet reconstruction, the measurement would be performed with muons produced in B-hadron decays. In particular selecting two same-sign muons in addition to the lepton from the $W$ boson provides a number of extremely interesting benefits:
\begin{itemize}
\item No intrinsic limitation in $\Delta R$ space, compared to the limit of $\Delta R=0.4$ in jets, or $\Delta R=0.2$ in track-jets. The superb spatial resolution of muons allows to probe a regime that has not been measured before
\item The study of the collinear regime provides a natural suppression of the \ttbar\ background. Based on MC studies the \ttbar\ background is expected to be less than 10\% of the signal.
\item A common limitation to measurements of heavy flavour production is the precise determination of the additional heavy-flavour backgrounds such as single $b$-jet with an additional mistagged jet, or pairs of charm jets. The requirement of muons with same electric charge can only be satisfied in events with two B-hadrons, where the first muon comes from a semileptonic B-hadron decay, and the second from a cascade of B-hadron to D-hadron and subsequent leptonic decay. Muon pairs from single B-hadron or two D-hadrons can only produce opposite-sign muons, except for rare cases of D-hadron oscillations.
\item The full event selection and measurement can be performed purely based on lepton observables, leading to a measurement with extremely low systematic uncertainties. 
\end{itemize}
}
\cvitem{}{
\begin{itemize}
\item A natural evolution of the analysis is the transition from two muons from the B-hadron decays, to two $J/\psi\rightarrow\mu\bar{\mu}$ decays. The extremely low branching ratio (BR($B\rightarrow J/\psi\rightarrow\mu\bar{\mu}=5\cdot 10^{-4}$) would not be an obstacle at HL-LHC, and provides an even cleaner final state with improved spatial correspondence between the B-hadron direction and the measured $J/\psi$.
\end{itemize}
}

\cvitem{}{
In addition, an analysis in this final state would have a large synergy with future top mass measurements exploiting the same $B\rightarrow J/\psi\rightarrow\mu\bar{\mu}$ decay, as way of avoiding the systematic uncertainties due to jet energy calibration and $b$-tagging.
}

%% ML
\cvitem{ML for lepton id.}{
While the increase in luminosity allows for a natural tightening of the lepton identification and isolation criteria, a high efficiency is in many cases still desirable, in order to minimise systematic uncertainties due to efficiency corrections. The rapid evolution of the machine-learning (ML) field has provided a magnificent set of tools that can be applied in high energy physics to boost the performance in multiple areas. In particular improvements to lepton identification and isolation will have a positive benefit to the full ATLAS program.
}
\cvitem{}{
Basic multi-variate techniques have been applied to electron identification, such as likelihood discriminants, based on crafted high-level features. Muon identification in the regime of interest for $W$-boson decays is based on simple cuts on a set of variables.
The potential for improvements is immense, and a targeted program in this area would bring a wide range of improvements.
\begin{itemize}
\item The set of variables used for identification can be extended to incorporate low-level data at the level of tracks or individual cells via convolutional neural networks. The inclusion of low-level features in addition to high-level features selected by physicist has been shown to consistently improve the separation in classification tasks.
\item The usage of a simple likelihood technique in electron identification, which neglects useful variable correlations, has its motivation in the poor modelling of MC simulation of electron shower shapes. This problem could become even more severe with the introduction of additional low-level features. A switch in paradigm would be required in order to train the ML discriminants directly on data, exploiting leptonic $Z$ decays via tag-and-probe, which provides a clean sample of unbiased leptons.
\item A certain mixture of backgrounds are the target of different requirements in the selection process. For example the electron identification aims at rejecting jets, isolation provides an additional rejection for non-prompt electrons, which might be followed by a material conversion veto, and electron charge misidentification. The factorisation of these steps is not always obvious, and changes to one of the criteria might strongly affect the performance of further steps. A unified approach where all steps are combined into a single multi-class classifier is a promising approach, where the optimal selection over the full set of backgrounds can be determined. A single criteria for identification plus isolation would in addition have access to information about the correlation of both, providing useful additional information towards the rejection of backgrounds.
\end{itemize}
}
\cvitem{}{
The modernisation of lepton identification and isolation selection has the potential to improve the performance of essential objects for the whole collaboration such as leptons. However the relatively straightforward application of modern techniques has to be carefully balanced against our capabilities to calibrate successfully the resulting selection. Such calibration and the tuning of a `lepton working point` would become tightly linked to the multi-lepton program.
}

%%need to fill a page of VIM in full screen...
%ML lepton identification + isolation
\cvitem{}{
As discussed, I consider that a program of measurements in final states with multiple leptons and $b$-jets has a large potential. In addition it provides the flexibility to fully exploit the fast increase in recorded dataset during the next years, and up to HL-LHC.
}
